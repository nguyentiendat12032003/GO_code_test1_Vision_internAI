# I. Introdution 
Text-to-Speech (TTS) is a technology in Artificial Intelligence (AI) and Natural Language Processing (NLP) that converts written text into spoken voice. With TTS, content from books, documents, websites, or messages can be automatically transformed into audio.
In practice, TTS has many important applications:
- Education and learning: supporting reading materials and providing accessibility for visually impaired users.
- Virtual assistants and chatbots: enabling more natural and human-like interactions.
- Entertainment and digital content: generating voice for videos, audiobooks, or automatic broadcasting.
- Customer service: powering automated call centers that can speak information to customers.
# II. Pipeline
To build a Vietnamese Text-to-Speech (TTS) system, the pipeline can be divided into four main stages: **Text Processing, Acoustic Modeling, Vocoder and Output**. Each stage plays an important role in converting raw text into natural-sounding audio.
To illustrate how the pipeline works, let us take the input text:

Input text:
```
“Hôm nay trời 21°C ở TPHCM và Hà Nội.”
```
## 1. Text Processing (Linguistic Frontend)
### Text normalization:
Raw text often contains numbers, symbols, abbreviations, or special characters that are not directly “speakable.” The TTS system needs normalized forms so it knows how to pronounce them naturally.

Example 1 (Numbers):
```
“21°C” -> “hai mươi mốt độ C”.
```
If we skip normalization, the system might read “21” as “hai một” (incorrect) instead of “hai mươi mốt”.

Example 2 (Abbreviations):
```
"TPHCM -> Thành phố Hồ Chí Minh"
```
If left unchanged, the system may pronounce each letter individually (“tê pê hờ xê mờ”), which is wrong.

Example 3 (Currency / Symbols):
```
"100K -> một trăm nghìn"
```
Without normalization, it could be read as “một trăm ka”, which is unnatural.

With the input text above, final normalized sentence:
```
“Hôm nay trời 21°C ở TPHCM -> Hôm nay trời hai mươi mốt độ C ở Thành phố Hồ Chí Minh và Hà Nội"
```
### Word segmentation:
Vietnamese uses spaces between syllables, not words. This is different from English. A naive system might mistakenly treat each syllable as a separate word, breaking meaning and pronunciation.
Syllables grouped into words: ``` [“Hôm nay”], [“trời”], [“hai mươi mốt”], [“độ C”], [“ở”], [“TPCHM”], ["Hà Nội"]. ```

Example:
```
"Hà Nội"
```
Without segmentation: [“Hà”], [“Nội”] → system may pause unnaturally between syllables.
With segmentation: [“Hà Nội”] → correct grouping as a single word (city name).
Example segmentation result for the input sentence:
```
“Hôm nay trời 21°C ở TPHCM và Hà Nội -> [“Hôm nay”], [“trời”], [“hai mươi mốt”], [“độ C”], [“ở”], ["Thành phố Hồ Chí Minh"], ["và"], [“Hà Nội”] ”
```
### Tone and prosody annotation:
Vietnamese is a tonal language with six tones (ngang, huyền, sắc, hỏi, ngã, nặng). Tones completely change meaning. If tone information is not preserved, TTS may produce speech that sounds confusing or incorrect.
Example (tones change meaning):
```
“ma” (ngang)
“má” (sắc)
“mà” (huyền)
“mã” (ngã)
“mả” (hỏi)
“mạ” (nặng)
```
All are spelled “ma” + different tones → six different words.
Example with sentence prosody:
```
“Hôm nay trời đẹp quá.”
```
Correct prosody: rising tone at “quá” (exclamation) makes the sentence sound expressive.

If prosody is missing, TTS may read it monotonously, losing naturalness.

Annotated the input sentence:

```“Hôm(ngang) nay(ngang) trời(huyền) hai(ngang) mươi(ngang) mốt(sắc) độ(nặng) C(ngang) ở(hỏi) Thành(huyền) phố(sắc) Hồ(huyền) Chí(sắc) Minh(ngang) và(huyền) Hà(huyền) Nội(nặng)”```
## 2. Acoustic Model (Text → Mel-Spectrogram)
After text processing, the normalized and tokenized text is passed to an acoustic model, which converts it into a mel-spectrogram.
### What is a mel-spectrogram?
It is a 2D representation of sound where:
- X-axis = time (speech progression).
- Y-axis = frequency (pitch/intonation).
Color/brightness = energy (how strong the sound is).
=> Think of it as an “image” of speech.
### How the model works:
Tacotron 2 (autoregressive) and FastSpeech 2 (non-autoregressive) learn the mapping between characters/phonemes + tone information → mel-spectrogram frames.
Example: For the sentence “Hôm nay trời đẹp”:
- “Hôm” (flat tone, ngang) → relatively stable frequency bands.
- “trời” (falling tone, huyền) → bands that slope downward in the spectrogram.
- “đẹp” (short, abrupt stop) → sharp cutoff in frequencies.
This step is crucial because it captures intonation, rhythm, and tones that determine whether Vietnamese speech sounds natural or incorrect.
## 3. Vocoder (Spectrogram → Audio Waveform)
The mel-spectrogram generated by the acoustic model must then be converted into an actual audio waveform. This step uses a neural vocoder, which synthesizes speech with high fidelity. Common choices include:
- **WaveNet:** autoregressive vocoder that produces high-quality audio but is slow.
- **WaveGlow or HiFi-GAN:** faster alternatives that achieve real-time or near-real-time synthesis while maintaining natural voice quality.
Here, the vocoder acts like a musical instrument that plays out the “sheet music” (mel-spectrogram) into real sound waves.
## 4. Output (Speech Generation)
Finally, the waveform is saved or streamed as an audio file, typically in .wav or .mp3 format. At this stage, additional features such as:
- Speaker identity (male/female, regional accents).
- Emotion control (neutral, happy, sad).
can also be added if the system is trained with a multi-speaker or expressive dataset.
# III. Algorithms / Models Used
To implement the Vietnamese TTS system, a combination of deep learning models is required, each responsible for a different stage of the pipeline.
## 1. Acoustic Models
- **Tacotron 2:** A sequence-to-sequence model with attention that converts text sequences into mel-spectrograms. It is well known for producing natural prosody and human-like intonation.
- **FastSpeech 2:** A non-autoregressive model that improves inference speed and stability. Unlike Tacotron 2, it does not rely on attention alignment, which reduces errors such as skipped or repeated words.
- **VITS (Variational Inference Text-to-Speech):** A recent end-to-end model that integrates acoustic modeling and vocoder into a single framework. It produces high-quality and more natural speech while reducing pipeline complexity.
## 2. Vocoder Models
- **WaveNet:** A pioneering autoregressive vocoder that generates high-quality audio at the cost of slow inference.
- **WaveGlow:** A flow-based model that can synthesize speech faster than WaveNet while maintaining good audio quality.
- **HiFi-GAN:** A state-of-the-art GAN-based vocoder that achieves both high naturalness and real-time synthesis, making it very suitable for practical Vietnamese TTS systems.
## 3. NLP Tools for Vietnamese
**VnCoreNLP or RDRSegmenter:** Tools for word segmentation and linguistic preprocessing in Vietnamese. Since Vietnamese uses spaces between syllables rather than words, these tools are essential for accurate text processing before passing input into the acoustic model.
Together, these models form the backbone of a modern TTS pipeline. The acoustic model learns the mapping from text to mel-spectrograms, while the vocoder transforms spectrograms into audible speech. Meanwhile, Vietnamese-specific NLP preprocessing tools ensure correct pronunciation and tone representation.
# IV. Problems & Solutions
## 1. Text Normalization Issues
**Problem:**
Numbers, dates, units, and abbreviations can have multiple spoken forms.

**Example:**
```
“21” could be read as “hai mươi mốt” or “hai mốt” (both correct, but one may be preferred).
“12/09/2025” could be “ngày mười hai tháng chín năm hai nghìn không trăm hai mươi lăm” or “một hai chín hai không hai năm”.
```
Vietnamese has regional variations (North vs South pronunciation).
Abbreviations are not always standardized (e.g, “TP” = “thành phố”).
**Solution:**

- Build rule-based normalization for common cases (numbers, dates, units, currency).
- Use custom dictionaries for domain-specific abbreviations (medical, finance, etc.).
- Add user configuration (e.g, northern vs southern reading preferences).
## 2. Word Segmentation Ambiguity
**Problem:**
Spaces separate syllables, not words → ambiguity in grouping.

**Example:**

```
“thuốc lá” = “cigarette”
If segmented wrong: [“thuốc”], [“lá”] = “medicine leaf” (nonsense).
```
Same syllables can form different words depending on context.

Vietnamese has many compound words.
**Solution:**
- Use NLP tools (VnCoreNLP) for statistical word segmentation.
- Improve with large Vietnamese corpus training.
- Apply post-processing rules to handle common mistakes (always group “Hà Nội” as one word).
## 3. Tone Ambiguity and Loss
**Problem:**
Tones must be preserved, but diacritics in digital text can be missing, inconsistent, or corrupted.

**Example:**
```
“ma” (ngang)
“má” (sắc)
“mà” (huyền)
“mã” (ngã)
“mả” (hỏi)
“mạ” (nặng)
```
“ma” could mean 6 different words depending on tone.
If diacritics are missing → TTS cannot disambiguate.
Many Vietnamese users type without diacritics (“hom nay troi dep qua” instead of “hôm nay trời đẹp quá”).
Automatic tone recovery is not 100% accurate.

**Solution:**
- Force diacritic check before processing.
- Use diacritic restoration models (sequence-to-sequence neural networks trained to add tones).
- In ambiguous cases, ask user clarification or choose most common meaning from context.
## 4. Dialect & Accent Variation
**Problem:**
Vietnamese has three major dialects: North, Central, South.

**Example:**
``
“anh ấy” (North) vs “ảnh” (South).
``
If trained only on Northern data, the system may sound unnatural to Southern users.
Collecting balanced datasets for all dialects is difficult.

**Solution:**
- Build multi-speaker, multi-dialect datasets.
- Use speaker embedding or dialect conditioning (let users choose preferred accent).
- Transfer learning: fine-tune base TTS model with small Southern/Central datasets.
# V. Conclusion
Building a Vietnamese TTS system requires careful handling of text normalization, word segmentation, tone preservation, and prosody modeling. By combining deep learning models (Tacotron 2, FastSpeech 2, VITS) with powerful vocoders (HiFi-GAN, WaveGlow) and Vietnamese-specific NLP preprocessing tools, we can generate natural and intelligible speech.

However, challenges such as text ambiguity, tonal complexity, and dialect variation remain. Addressing these with rule-based normalization, diacritic restoration, and multi-dialect training ensures a more robust and inclusive TTS system.
